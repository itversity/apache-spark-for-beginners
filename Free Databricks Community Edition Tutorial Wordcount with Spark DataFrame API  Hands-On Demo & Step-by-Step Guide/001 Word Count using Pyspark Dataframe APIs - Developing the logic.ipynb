{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e40a32-baab-4f56-933e-5666dd16b44f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1455043490000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/SPARK_README.md",
         "SPARK_README.md",
         3359,
         1455043490000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/databricks-datasets/SPARK_README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1bc727-dfc8-4ed6-8335-69b8b7b09421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lines = spark.read.text('dbfs:/databricks-datasets/SPARK_README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07698e35-a4c5-42d9-95d1-52719b44a3c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td># Apache Spark</td></tr><tr><td></td></tr><tr><td>Spark is a fast and general cluster computing system for Big Data. It provides</td></tr><tr><td>high-level APIs in Scala, Java, Python, and R, and an optimized engine that</td></tr><tr><td>supports general computation graphs for data analysis. It also supports a</td></tr><tr><td>rich set of higher-level tools including Spark SQL for SQL and DataFrames,</td></tr><tr><td>MLlib for machine learning, GraphX for graph processing,</td></tr><tr><td>and Spark Streaming for stream processing.</td></tr><tr><td></td></tr><tr><td><http://spark.apache.org/></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td>## Online Documentation</td></tr><tr><td></td></tr><tr><td>You can find the latest Spark documentation, including a programming</td></tr><tr><td>guide, on the [project web page](http://spark.apache.org/documentation.html)</td></tr><tr><td>and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).</td></tr><tr><td>This README file only contains basic setup instructions.</td></tr><tr><td></td></tr><tr><td>## Building Spark</td></tr><tr><td></td></tr><tr><td>Spark is built using [Apache Maven](http://maven.apache.org/).</td></tr><tr><td>To build Spark and its example programs, run:</td></tr><tr><td></td></tr><tr><td>    build/mvn -DskipTests clean package</td></tr><tr><td></td></tr><tr><td>(You do not need to do this if you downloaded a pre-built package.)</td></tr><tr><td>More detailed documentation is available from the project site, at</td></tr><tr><td>[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).</td></tr><tr><td></td></tr><tr><td>## Interactive Scala Shell</td></tr><tr><td></td></tr><tr><td>The easiest way to start using Spark is through the Scala shell:</td></tr><tr><td></td></tr><tr><td>    ./bin/spark-shell</td></tr><tr><td></td></tr><tr><td>Try the following command, which should return 1000:</td></tr><tr><td></td></tr><tr><td>    scala> sc.parallelize(1 to 1000).count()</td></tr><tr><td></td></tr><tr><td>## Interactive Python Shell</td></tr><tr><td></td></tr><tr><td>Alternatively, if you prefer Python, you can use the Python shell:</td></tr><tr><td></td></tr><tr><td>    ./bin/pyspark</td></tr><tr><td></td></tr><tr><td>And run the following command, which should also return 1000:</td></tr><tr><td></td></tr><tr><td>    >>> sc.parallelize(range(1000)).count()</td></tr><tr><td></td></tr><tr><td>## Example Programs</td></tr><tr><td></td></tr><tr><td>Spark also comes with several sample programs in the `examples` directory.</td></tr><tr><td>To run one of them, use `./bin/run-example <class> [params]`. For example:</td></tr><tr><td></td></tr><tr><td>    ./bin/run-example SparkPi</td></tr><tr><td></td></tr><tr><td>will run the Pi example locally.</td></tr><tr><td></td></tr><tr><td>You can set the MASTER environment variable when running examples to submit</td></tr><tr><td>examples to a cluster. This can be a mesos:// or spark:// URL,</td></tr><tr><td>\"yarn\" to run on YARN, and \"local\" to run</td></tr><tr><td>locally with one thread, or \"local[N]\" to run locally with N threads. You</td></tr><tr><td>can also use an abbreviated class name if the class is in the `examples`</td></tr><tr><td>package. For instance:</td></tr><tr><td></td></tr><tr><td>    MASTER=spark://host:7077 ./bin/run-example SparkPi</td></tr><tr><td></td></tr><tr><td>Many of the example programs print usage help if no params are given.</td></tr><tr><td></td></tr><tr><td>## Running Tests</td></tr><tr><td></td></tr><tr><td>Testing first requires [building Spark](#building-spark). Once Spark is built, tests</td></tr><tr><td>can be run using:</td></tr><tr><td></td></tr><tr><td>    ./dev/run-tests</td></tr><tr><td></td></tr><tr><td>Please see the guidance on how to</td></tr><tr><td>[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).</td></tr><tr><td></td></tr><tr><td>## A Note About Hadoop Versions</td></tr><tr><td></td></tr><tr><td>Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported</td></tr><tr><td>storage systems. Because the protocols have changed in different versions of</td></tr><tr><td>Hadoop, you must build Spark against the same version that your cluster runs.</td></tr><tr><td></td></tr><tr><td>Please refer to the build documentation at</td></tr><tr><td>[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)</td></tr><tr><td>for detailed guidance on building for a particular distribution of Hadoop, including</td></tr><tr><td>building for particular Hive and Hive Thriftserver distributions.</td></tr><tr><td></td></tr><tr><td>## Configuration</td></tr><tr><td></td></tr><tr><td>Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)</td></tr><tr><td>in the online documentation for an overview on how to configure Spark.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "# Apache Spark"
        ],
        [
         ""
        ],
        [
         "Spark is a fast and general cluster computing system for Big Data. It provides"
        ],
        [
         "high-level APIs in Scala, Java, Python, and R, and an optimized engine that"
        ],
        [
         "supports general computation graphs for data analysis. It also supports a"
        ],
        [
         "rich set of higher-level tools including Spark SQL for SQL and DataFrames,"
        ],
        [
         "MLlib for machine learning, GraphX for graph processing,"
        ],
        [
         "and Spark Streaming for stream processing."
        ],
        [
         ""
        ],
        [
         "<http://spark.apache.org/>"
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         "## Online Documentation"
        ],
        [
         ""
        ],
        [
         "You can find the latest Spark documentation, including a programming"
        ],
        [
         "guide, on the [project web page](http://spark.apache.org/documentation.html)"
        ],
        [
         "and [project wiki](https://cwiki.apache.org/confluence/display/SPARK)."
        ],
        [
         "This README file only contains basic setup instructions."
        ],
        [
         ""
        ],
        [
         "## Building Spark"
        ],
        [
         ""
        ],
        [
         "Spark is built using [Apache Maven](http://maven.apache.org/)."
        ],
        [
         "To build Spark and its example programs, run:"
        ],
        [
         ""
        ],
        [
         "    build/mvn -DskipTests clean package"
        ],
        [
         ""
        ],
        [
         "(You do not need to do this if you downloaded a pre-built package.)"
        ],
        [
         "More detailed documentation is available from the project site, at"
        ],
        [
         "[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html)."
        ],
        [
         ""
        ],
        [
         "## Interactive Scala Shell"
        ],
        [
         ""
        ],
        [
         "The easiest way to start using Spark is through the Scala shell:"
        ],
        [
         ""
        ],
        [
         "    ./bin/spark-shell"
        ],
        [
         ""
        ],
        [
         "Try the following command, which should return 1000:"
        ],
        [
         ""
        ],
        [
         "    scala> sc.parallelize(1 to 1000).count()"
        ],
        [
         ""
        ],
        [
         "## Interactive Python Shell"
        ],
        [
         ""
        ],
        [
         "Alternatively, if you prefer Python, you can use the Python shell:"
        ],
        [
         ""
        ],
        [
         "    ./bin/pyspark"
        ],
        [
         ""
        ],
        [
         "And run the following command, which should also return 1000:"
        ],
        [
         ""
        ],
        [
         "    >>> sc.parallelize(range(1000)).count()"
        ],
        [
         ""
        ],
        [
         "## Example Programs"
        ],
        [
         ""
        ],
        [
         "Spark also comes with several sample programs in the `examples` directory."
        ],
        [
         "To run one of them, use `./bin/run-example <class> [params]`. For example:"
        ],
        [
         ""
        ],
        [
         "    ./bin/run-example SparkPi"
        ],
        [
         ""
        ],
        [
         "will run the Pi example locally."
        ],
        [
         ""
        ],
        [
         "You can set the MASTER environment variable when running examples to submit"
        ],
        [
         "examples to a cluster. This can be a mesos:// or spark:// URL,"
        ],
        [
         "\"yarn\" to run on YARN, and \"local\" to run"
        ],
        [
         "locally with one thread, or \"local[N]\" to run locally with N threads. You"
        ],
        [
         "can also use an abbreviated class name if the class is in the `examples`"
        ],
        [
         "package. For instance:"
        ],
        [
         ""
        ],
        [
         "    MASTER=spark://host:7077 ./bin/run-example SparkPi"
        ],
        [
         ""
        ],
        [
         "Many of the example programs print usage help if no params are given."
        ],
        [
         ""
        ],
        [
         "## Running Tests"
        ],
        [
         ""
        ],
        [
         "Testing first requires [building Spark](#building-spark). Once Spark is built, tests"
        ],
        [
         "can be run using:"
        ],
        [
         ""
        ],
        [
         "    ./dev/run-tests"
        ],
        [
         ""
        ],
        [
         "Please see the guidance on how to"
        ],
        [
         "[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools)."
        ],
        [
         ""
        ],
        [
         "## A Note About Hadoop Versions"
        ],
        [
         ""
        ],
        [
         "Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported"
        ],
        [
         "storage systems. Because the protocols have changed in different versions of"
        ],
        [
         "Hadoop, you must build Spark against the same version that your cluster runs."
        ],
        [
         ""
        ],
        [
         "Please refer to the build documentation at"
        ],
        [
         "[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)"
        ],
        [
         "for detailed guidance on building for a particular distribution of Hadoop, including"
        ],
        [
         "building for particular Hive and Hive Thriftserver distributions."
        ],
        [
         ""
        ],
        [
         "## Configuration"
        ],
        [
         ""
        ],
        [
         "Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)"
        ],
        [
         "in the online documentation for an overview on how to configure Spark."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lines.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2493fc2-55ba-4335-bb37-f39ae581d226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n|value                                                                         |\n+------------------------------------------------------------------------------+\n|# Apache Spark                                                                |\n|                                                                              |\n|Spark is a fast and general cluster computing system for Big Data. It provides|\n|high-level APIs in Scala, Java, Python, and R, and an optimized engine that   |\n|supports general computation graphs for data analysis. It also supports a     |\n|rich set of higher-level tools including Spark SQL for SQL and DataFrames,    |\n|MLlib for machine learning, GraphX for graph processing,                      |\n|and Spark Streaming for stream processing.                                    |\n|                                                                              |\n|<http://spark.apache.org/>                                                    |\n|                                                                              |\n|                                                                              |\n|## Online Documentation                                                       |\n|                                                                              |\n|You can find the latest Spark documentation, including a programming          |\n|guide, on the [project web page](http://spark.apache.org/documentation.html)  |\n|and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).        |\n|This README file only contains basic setup instructions.                      |\n|                                                                              |\n|## Building Spark                                                             |\n+------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "lines.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a591bbaa-de2e-4c6a-8771-9be2b525d6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d682ef-efea-4950-b9d3-b597cfb4382f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- value: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "675f9397-34e6-48f9-8de1-3b4d583659ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td># Apache Spark</td></tr><tr><td>Spark is a fast and general cluster computing system for Big Data. It provides</td></tr><tr><td>high-level APIs in Scala, Java, Python, and R, and an optimized engine that</td></tr><tr><td>supports general computation graphs for data analysis. It also supports a</td></tr><tr><td>rich set of higher-level tools including Spark SQL for SQL and DataFrames,</td></tr><tr><td>MLlib for machine learning, GraphX for graph processing,</td></tr><tr><td>and Spark Streaming for stream processing.</td></tr><tr><td><http://spark.apache.org/></td></tr><tr><td>## Online Documentation</td></tr><tr><td>You can find the latest Spark documentation, including a programming</td></tr><tr><td>guide, on the [project web page](http://spark.apache.org/documentation.html)</td></tr><tr><td>and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).</td></tr><tr><td>This README file only contains basic setup instructions.</td></tr><tr><td>## Building Spark</td></tr><tr><td>Spark is built using [Apache Maven](http://maven.apache.org/).</td></tr><tr><td>To build Spark and its example programs, run:</td></tr><tr><td>    build/mvn -DskipTests clean package</td></tr><tr><td>(You do not need to do this if you downloaded a pre-built package.)</td></tr><tr><td>More detailed documentation is available from the project site, at</td></tr><tr><td>[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).</td></tr><tr><td>## Interactive Scala Shell</td></tr><tr><td>The easiest way to start using Spark is through the Scala shell:</td></tr><tr><td>    ./bin/spark-shell</td></tr><tr><td>Try the following command, which should return 1000:</td></tr><tr><td>    scala> sc.parallelize(1 to 1000).count()</td></tr><tr><td>## Interactive Python Shell</td></tr><tr><td>Alternatively, if you prefer Python, you can use the Python shell:</td></tr><tr><td>    ./bin/pyspark</td></tr><tr><td>And run the following command, which should also return 1000:</td></tr><tr><td>    >>> sc.parallelize(range(1000)).count()</td></tr><tr><td>## Example Programs</td></tr><tr><td>Spark also comes with several sample programs in the `examples` directory.</td></tr><tr><td>To run one of them, use `./bin/run-example <class> [params]`. For example:</td></tr><tr><td>    ./bin/run-example SparkPi</td></tr><tr><td>will run the Pi example locally.</td></tr><tr><td>You can set the MASTER environment variable when running examples to submit</td></tr><tr><td>examples to a cluster. This can be a mesos:// or spark:// URL,</td></tr><tr><td>\"yarn\" to run on YARN, and \"local\" to run</td></tr><tr><td>locally with one thread, or \"local[N]\" to run locally with N threads. You</td></tr><tr><td>can also use an abbreviated class name if the class is in the `examples`</td></tr><tr><td>package. For instance:</td></tr><tr><td>    MASTER=spark://host:7077 ./bin/run-example SparkPi</td></tr><tr><td>Many of the example programs print usage help if no params are given.</td></tr><tr><td>## Running Tests</td></tr><tr><td>Testing first requires [building Spark](#building-spark). Once Spark is built, tests</td></tr><tr><td>can be run using:</td></tr><tr><td>    ./dev/run-tests</td></tr><tr><td>Please see the guidance on how to</td></tr><tr><td>[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).</td></tr><tr><td>## A Note About Hadoop Versions</td></tr><tr><td>Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported</td></tr><tr><td>storage systems. Because the protocols have changed in different versions of</td></tr><tr><td>Hadoop, you must build Spark against the same version that your cluster runs.</td></tr><tr><td>Please refer to the build documentation at</td></tr><tr><td>[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)</td></tr><tr><td>for detailed guidance on building for a particular distribution of Hadoop, including</td></tr><tr><td>building for particular Hive and Hive Thriftserver distributions.</td></tr><tr><td>## Configuration</td></tr><tr><td>Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)</td></tr><tr><td>in the online documentation for an overview on how to configure Spark.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "# Apache Spark"
        ],
        [
         "Spark is a fast and general cluster computing system for Big Data. It provides"
        ],
        [
         "high-level APIs in Scala, Java, Python, and R, and an optimized engine that"
        ],
        [
         "supports general computation graphs for data analysis. It also supports a"
        ],
        [
         "rich set of higher-level tools including Spark SQL for SQL and DataFrames,"
        ],
        [
         "MLlib for machine learning, GraphX for graph processing,"
        ],
        [
         "and Spark Streaming for stream processing."
        ],
        [
         "<http://spark.apache.org/>"
        ],
        [
         "## Online Documentation"
        ],
        [
         "You can find the latest Spark documentation, including a programming"
        ],
        [
         "guide, on the [project web page](http://spark.apache.org/documentation.html)"
        ],
        [
         "and [project wiki](https://cwiki.apache.org/confluence/display/SPARK)."
        ],
        [
         "This README file only contains basic setup instructions."
        ],
        [
         "## Building Spark"
        ],
        [
         "Spark is built using [Apache Maven](http://maven.apache.org/)."
        ],
        [
         "To build Spark and its example programs, run:"
        ],
        [
         "    build/mvn -DskipTests clean package"
        ],
        [
         "(You do not need to do this if you downloaded a pre-built package.)"
        ],
        [
         "More detailed documentation is available from the project site, at"
        ],
        [
         "[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html)."
        ],
        [
         "## Interactive Scala Shell"
        ],
        [
         "The easiest way to start using Spark is through the Scala shell:"
        ],
        [
         "    ./bin/spark-shell"
        ],
        [
         "Try the following command, which should return 1000:"
        ],
        [
         "    scala> sc.parallelize(1 to 1000).count()"
        ],
        [
         "## Interactive Python Shell"
        ],
        [
         "Alternatively, if you prefer Python, you can use the Python shell:"
        ],
        [
         "    ./bin/pyspark"
        ],
        [
         "And run the following command, which should also return 1000:"
        ],
        [
         "    >>> sc.parallelize(range(1000)).count()"
        ],
        [
         "## Example Programs"
        ],
        [
         "Spark also comes with several sample programs in the `examples` directory."
        ],
        [
         "To run one of them, use `./bin/run-example <class> [params]`. For example:"
        ],
        [
         "    ./bin/run-example SparkPi"
        ],
        [
         "will run the Pi example locally."
        ],
        [
         "You can set the MASTER environment variable when running examples to submit"
        ],
        [
         "examples to a cluster. This can be a mesos:// or spark:// URL,"
        ],
        [
         "\"yarn\" to run on YARN, and \"local\" to run"
        ],
        [
         "locally with one thread, or \"local[N]\" to run locally with N threads. You"
        ],
        [
         "can also use an abbreviated class name if the class is in the `examples`"
        ],
        [
         "package. For instance:"
        ],
        [
         "    MASTER=spark://host:7077 ./bin/run-example SparkPi"
        ],
        [
         "Many of the example programs print usage help if no params are given."
        ],
        [
         "## Running Tests"
        ],
        [
         "Testing first requires [building Spark](#building-spark). Once Spark is built, tests"
        ],
        [
         "can be run using:"
        ],
        [
         "    ./dev/run-tests"
        ],
        [
         "Please see the guidance on how to"
        ],
        [
         "[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools)."
        ],
        [
         "## A Note About Hadoop Versions"
        ],
        [
         "Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported"
        ],
        [
         "storage systems. Because the protocols have changed in different versions of"
        ],
        [
         "Hadoop, you must build Spark against the same version that your cluster runs."
        ],
        [
         "Please refer to the build documentation at"
        ],
        [
         "[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)"
        ],
        [
         "for detailed guidance on building for a particular distribution of Hadoop, including"
        ],
        [
         "building for particular Hive and Hive Thriftserver distributions."
        ],
        [
         "## Configuration"
        ],
        [
         "Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)"
        ],
        [
         "in the online documentation for an overview on how to configure Spark."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lines.filter(\"value != ''\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0ae0ed-9bc3-4f4e-8a21-5071b96910ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43fb43b-f520-4901-97c1-56eb9f95caa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "words = lines.filter(\"value != ''\"). \\\n",
    "    select(explode(split('value', ' ')).alias('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae71b191-d604-40ea-80e8-47a63de96acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>word</th></tr></thead><tbody><tr><td>#</td></tr><tr><td>Apache</td></tr><tr><td>Spark</td></tr><tr><td>Spark</td></tr><tr><td>is</td></tr><tr><td>a</td></tr><tr><td>fast</td></tr><tr><td>and</td></tr><tr><td>general</td></tr><tr><td>cluster</td></tr><tr><td>computing</td></tr><tr><td>system</td></tr><tr><td>for</td></tr><tr><td>Big</td></tr><tr><td>Data.</td></tr><tr><td>It</td></tr><tr><td>provides</td></tr><tr><td>high-level</td></tr><tr><td>APIs</td></tr><tr><td>in</td></tr><tr><td>Scala,</td></tr><tr><td>Java,</td></tr><tr><td>Python,</td></tr><tr><td>and</td></tr><tr><td>R,</td></tr><tr><td>and</td></tr><tr><td>an</td></tr><tr><td>optimized</td></tr><tr><td>engine</td></tr><tr><td>that</td></tr><tr><td>supports</td></tr><tr><td>general</td></tr><tr><td>computation</td></tr><tr><td>graphs</td></tr><tr><td>for</td></tr><tr><td>data</td></tr><tr><td>analysis.</td></tr><tr><td>It</td></tr><tr><td>also</td></tr><tr><td>supports</td></tr><tr><td>a</td></tr><tr><td>rich</td></tr><tr><td>set</td></tr><tr><td>of</td></tr><tr><td>higher-level</td></tr><tr><td>tools</td></tr><tr><td>including</td></tr><tr><td>Spark</td></tr><tr><td>SQL</td></tr><tr><td>for</td></tr><tr><td>SQL</td></tr><tr><td>and</td></tr><tr><td>DataFrames,</td></tr><tr><td>MLlib</td></tr><tr><td>for</td></tr><tr><td>machine</td></tr><tr><td>learning,</td></tr><tr><td>GraphX</td></tr><tr><td>for</td></tr><tr><td>graph</td></tr><tr><td>processing,</td></tr><tr><td>and</td></tr><tr><td>Spark</td></tr><tr><td>Streaming</td></tr><tr><td>for</td></tr><tr><td>stream</td></tr><tr><td>processing.</td></tr><tr><td><http://spark.apache.org/></td></tr><tr><td>##</td></tr><tr><td>Online</td></tr><tr><td>Documentation</td></tr><tr><td>You</td></tr><tr><td>can</td></tr><tr><td>find</td></tr><tr><td>the</td></tr><tr><td>latest</td></tr><tr><td>Spark</td></tr><tr><td>documentation,</td></tr><tr><td>including</td></tr><tr><td>a</td></tr><tr><td>programming</td></tr><tr><td>guide,</td></tr><tr><td>on</td></tr><tr><td>the</td></tr><tr><td>[project</td></tr><tr><td>web</td></tr><tr><td>page](http://spark.apache.org/documentation.html)</td></tr><tr><td>and</td></tr><tr><td>[project</td></tr><tr><td>wiki](https://cwiki.apache.org/confluence/display/SPARK).</td></tr><tr><td>This</td></tr><tr><td>README</td></tr><tr><td>file</td></tr><tr><td>only</td></tr><tr><td>contains</td></tr><tr><td>basic</td></tr><tr><td>setup</td></tr><tr><td>instructions.</td></tr><tr><td>##</td></tr><tr><td>Building</td></tr><tr><td>Spark</td></tr><tr><td>Spark</td></tr><tr><td>is</td></tr><tr><td>built</td></tr><tr><td>using</td></tr><tr><td>[Apache</td></tr><tr><td>Maven](http://maven.apache.org/).</td></tr><tr><td>To</td></tr><tr><td>build</td></tr><tr><td>Spark</td></tr><tr><td>and</td></tr><tr><td>its</td></tr><tr><td>example</td></tr><tr><td>programs,</td></tr><tr><td>run:</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td>build/mvn</td></tr><tr><td>-DskipTests</td></tr><tr><td>clean</td></tr><tr><td>package</td></tr><tr><td>(You</td></tr><tr><td>do</td></tr><tr><td>not</td></tr><tr><td>need</td></tr><tr><td>to</td></tr><tr><td>do</td></tr><tr><td>this</td></tr><tr><td>if</td></tr><tr><td>you</td></tr><tr><td>downloaded</td></tr><tr><td>a</td></tr><tr><td>pre-built</td></tr><tr><td>package.)</td></tr><tr><td>More</td></tr><tr><td>detailed</td></tr><tr><td>documentation</td></tr><tr><td>is</td></tr><tr><td>available</td></tr><tr><td>from</td></tr><tr><td>the</td></tr><tr><td>project</td></tr><tr><td>site,</td></tr><tr><td>at</td></tr><tr><td>[\"Building</td></tr><tr><td>Spark\"](http://spark.apache.org/docs/latest/building-spark.html).</td></tr><tr><td>##</td></tr><tr><td>Interactive</td></tr><tr><td>Scala</td></tr><tr><td>Shell</td></tr><tr><td>The</td></tr><tr><td>easiest</td></tr><tr><td>way</td></tr><tr><td>to</td></tr><tr><td>start</td></tr><tr><td>using</td></tr><tr><td>Spark</td></tr><tr><td>is</td></tr><tr><td>through</td></tr><tr><td>the</td></tr><tr><td>Scala</td></tr><tr><td>shell:</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td>./bin/spark-shell</td></tr><tr><td>Try</td></tr><tr><td>the</td></tr><tr><td>following</td></tr><tr><td>command,</td></tr><tr><td>which</td></tr><tr><td>should</td></tr><tr><td>return</td></tr><tr><td>1000:</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td>scala></td></tr><tr><td>sc.parallelize(1</td></tr><tr><td>to</td></tr><tr><td>1000).count()</td></tr><tr><td>##</td></tr><tr><td>Interactive</td></tr><tr><td>Python</td></tr><tr><td>Shell</td></tr><tr><td>Alternatively,</td></tr><tr><td>if</td></tr><tr><td>you</td></tr><tr><td>prefer</td></tr><tr><td>Python,</td></tr><tr><td>you</td></tr><tr><td>can</td></tr><tr><td>use</td></tr><tr><td>the</td></tr><tr><td>Python</td></tr><tr><td>shell:</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td>./bin/pyspark</td></tr><tr><td>And</td></tr><tr><td>run</td></tr><tr><td>the</td></tr><tr><td>following</td></tr><tr><td>command,</td></tr><tr><td>which</td></tr><tr><td>should</td></tr><tr><td>also</td></tr><tr><td>return</td></tr><tr><td>1000:</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td>>>></td></tr><tr><td>sc.parallelize(range(1000)).count()</td></tr><tr><td>##</td></tr><tr><td>Example</td></tr><tr><td>Programs</td></tr><tr><td>Spark</td></tr><tr><td>also</td></tr><tr><td>comes</td></tr><tr><td>with</td></tr><tr><td>several</td></tr><tr><td>sample</td></tr><tr><td>programs</td></tr><tr><td>in</td></tr><tr><td>the</td></tr><tr><td>`examples`</td></tr><tr><td>directory.</td></tr><tr><td>To</td></tr><tr><td>run</td></tr><tr><td>one</td></tr><tr><td>of</td></tr><tr><td>them,</td></tr><tr><td>use</td></tr><tr><td>`./bin/run-example</td></tr><tr><td><class></td></tr><tr><td>[params]`.</td></tr><tr><td>For</td></tr><tr><td>example:</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td>./bin/run-example</td></tr><tr><td>SparkPi</td></tr><tr><td>will</td></tr><tr><td>run</td></tr><tr><td>the</td></tr><tr><td>Pi</td></tr><tr><td>example</td></tr><tr><td>locally.</td></tr><tr><td>You</td></tr><tr><td>can</td></tr><tr><td>set</td></tr><tr><td>the</td></tr><tr><td>MASTER</td></tr><tr><td>environment</td></tr><tr><td>variable</td></tr><tr><td>when</td></tr><tr><td>running</td></tr><tr><td>examples</td></tr><tr><td>to</td></tr><tr><td>submit</td></tr><tr><td>examples</td></tr><tr><td>to</td></tr><tr><td>a</td></tr><tr><td>cluster.</td></tr><tr><td>This</td></tr><tr><td>can</td></tr><tr><td>be</td></tr><tr><td>a</td></tr><tr><td>mesos://</td></tr><tr><td>or</td></tr><tr><td>spark://</td></tr><tr><td>URL,</td></tr><tr><td>\"yarn\"</td></tr><tr><td>to</td></tr><tr><td>run</td></tr><tr><td>on</td></tr><tr><td>YARN,</td></tr><tr><td>and</td></tr><tr><td>\"local\"</td></tr><tr><td>to</td></tr><tr><td>run</td></tr><tr><td>locally</td></tr><tr><td>with</td></tr><tr><td>one</td></tr><tr><td>thread,</td></tr><tr><td>or</td></tr><tr><td>\"local[N]\"</td></tr><tr><td>to</td></tr><tr><td>run</td></tr><tr><td>locally</td></tr><tr><td>with</td></tr><tr><td>N</td></tr><tr><td>threads.</td></tr><tr><td>You</td></tr><tr><td>can</td></tr><tr><td>also</td></tr><tr><td>use</td></tr><tr><td>an</td></tr><tr><td>abbreviated</td></tr><tr><td>class</td></tr><tr><td>name</td></tr><tr><td>if</td></tr><tr><td>the</td></tr><tr><td>class</td></tr><tr><td>is</td></tr><tr><td>in</td></tr><tr><td>the</td></tr><tr><td>`examples`</td></tr><tr><td>package.</td></tr><tr><td>For</td></tr><tr><td>instance:</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td>MASTER=spark://host:7077</td></tr><tr><td>./bin/run-example</td></tr><tr><td>SparkPi</td></tr><tr><td>Many</td></tr><tr><td>of</td></tr><tr><td>the</td></tr><tr><td>example</td></tr><tr><td>programs</td></tr><tr><td>print</td></tr><tr><td>usage</td></tr><tr><td>help</td></tr><tr><td>if</td></tr><tr><td>no</td></tr><tr><td>params</td></tr><tr><td>are</td></tr><tr><td>given.</td></tr><tr><td>##</td></tr><tr><td>Running</td></tr><tr><td>Tests</td></tr><tr><td>Testing</td></tr><tr><td>first</td></tr><tr><td>requires</td></tr><tr><td>[building</td></tr><tr><td>Spark](#building-spark).</td></tr><tr><td>Once</td></tr><tr><td>Spark</td></tr><tr><td>is</td></tr><tr><td>built,</td></tr><tr><td>tests</td></tr><tr><td>can</td></tr><tr><td>be</td></tr><tr><td>run</td></tr><tr><td>using:</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td>./dev/run-tests</td></tr><tr><td>Please</td></tr><tr><td>see</td></tr><tr><td>the</td></tr><tr><td>guidance</td></tr><tr><td>on</td></tr><tr><td>how</td></tr><tr><td>to</td></tr><tr><td>[run</td></tr><tr><td>tests</td></tr><tr><td>for</td></tr><tr><td>a</td></tr><tr><td>module,</td></tr><tr><td>or</td></tr><tr><td>individual</td></tr><tr><td>tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).</td></tr><tr><td>##</td></tr><tr><td>A</td></tr><tr><td>Note</td></tr><tr><td>About</td></tr><tr><td>Hadoop</td></tr><tr><td>Versions</td></tr><tr><td>Spark</td></tr><tr><td>uses</td></tr><tr><td>the</td></tr><tr><td>Hadoop</td></tr><tr><td>core</td></tr><tr><td>library</td></tr><tr><td>to</td></tr><tr><td>talk</td></tr><tr><td>to</td></tr><tr><td>HDFS</td></tr><tr><td>and</td></tr><tr><td>other</td></tr><tr><td>Hadoop-supported</td></tr><tr><td>storage</td></tr><tr><td>systems.</td></tr><tr><td>Because</td></tr><tr><td>the</td></tr><tr><td>protocols</td></tr><tr><td>have</td></tr><tr><td>changed</td></tr><tr><td>in</td></tr><tr><td>different</td></tr><tr><td>versions</td></tr><tr><td>of</td></tr><tr><td>Hadoop,</td></tr><tr><td>you</td></tr><tr><td>must</td></tr><tr><td>build</td></tr><tr><td>Spark</td></tr><tr><td>against</td></tr><tr><td>the</td></tr><tr><td>same</td></tr><tr><td>version</td></tr><tr><td>that</td></tr><tr><td>your</td></tr><tr><td>cluster</td></tr><tr><td>runs.</td></tr><tr><td>Please</td></tr><tr><td>refer</td></tr><tr><td>to</td></tr><tr><td>the</td></tr><tr><td>build</td></tr><tr><td>documentation</td></tr><tr><td>at</td></tr><tr><td>[\"Specifying</td></tr><tr><td>the</td></tr><tr><td>Hadoop</td></tr><tr><td>Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)</td></tr><tr><td>for</td></tr><tr><td>detailed</td></tr><tr><td>guidance</td></tr><tr><td>on</td></tr><tr><td>building</td></tr><tr><td>for</td></tr><tr><td>a</td></tr><tr><td>particular</td></tr><tr><td>distribution</td></tr><tr><td>of</td></tr><tr><td>Hadoop,</td></tr><tr><td>including</td></tr><tr><td>building</td></tr><tr><td>for</td></tr><tr><td>particular</td></tr><tr><td>Hive</td></tr><tr><td>and</td></tr><tr><td>Hive</td></tr><tr><td>Thriftserver</td></tr><tr><td>distributions.</td></tr><tr><td>##</td></tr><tr><td>Configuration</td></tr><tr><td>Please</td></tr><tr><td>refer</td></tr><tr><td>to</td></tr><tr><td>the</td></tr><tr><td>[Configuration</td></tr><tr><td>Guide](http://spark.apache.org/docs/latest/configuration.html)</td></tr><tr><td>in</td></tr><tr><td>the</td></tr><tr><td>online</td></tr><tr><td>documentation</td></tr><tr><td>for</td></tr><tr><td>an</td></tr><tr><td>overview</td></tr><tr><td>on</td></tr><tr><td>how</td></tr><tr><td>to</td></tr><tr><td>configure</td></tr><tr><td>Spark.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "#"
        ],
        [
         "Apache"
        ],
        [
         "Spark"
        ],
        [
         "Spark"
        ],
        [
         "is"
        ],
        [
         "a"
        ],
        [
         "fast"
        ],
        [
         "and"
        ],
        [
         "general"
        ],
        [
         "cluster"
        ],
        [
         "computing"
        ],
        [
         "system"
        ],
        [
         "for"
        ],
        [
         "Big"
        ],
        [
         "Data."
        ],
        [
         "It"
        ],
        [
         "provides"
        ],
        [
         "high-level"
        ],
        [
         "APIs"
        ],
        [
         "in"
        ],
        [
         "Scala,"
        ],
        [
         "Java,"
        ],
        [
         "Python,"
        ],
        [
         "and"
        ],
        [
         "R,"
        ],
        [
         "and"
        ],
        [
         "an"
        ],
        [
         "optimized"
        ],
        [
         "engine"
        ],
        [
         "that"
        ],
        [
         "supports"
        ],
        [
         "general"
        ],
        [
         "computation"
        ],
        [
         "graphs"
        ],
        [
         "for"
        ],
        [
         "data"
        ],
        [
         "analysis."
        ],
        [
         "It"
        ],
        [
         "also"
        ],
        [
         "supports"
        ],
        [
         "a"
        ],
        [
         "rich"
        ],
        [
         "set"
        ],
        [
         "of"
        ],
        [
         "higher-level"
        ],
        [
         "tools"
        ],
        [
         "including"
        ],
        [
         "Spark"
        ],
        [
         "SQL"
        ],
        [
         "for"
        ],
        [
         "SQL"
        ],
        [
         "and"
        ],
        [
         "DataFrames,"
        ],
        [
         "MLlib"
        ],
        [
         "for"
        ],
        [
         "machine"
        ],
        [
         "learning,"
        ],
        [
         "GraphX"
        ],
        [
         "for"
        ],
        [
         "graph"
        ],
        [
         "processing,"
        ],
        [
         "and"
        ],
        [
         "Spark"
        ],
        [
         "Streaming"
        ],
        [
         "for"
        ],
        [
         "stream"
        ],
        [
         "processing."
        ],
        [
         "<http://spark.apache.org/>"
        ],
        [
         "##"
        ],
        [
         "Online"
        ],
        [
         "Documentation"
        ],
        [
         "You"
        ],
        [
         "can"
        ],
        [
         "find"
        ],
        [
         "the"
        ],
        [
         "latest"
        ],
        [
         "Spark"
        ],
        [
         "documentation,"
        ],
        [
         "including"
        ],
        [
         "a"
        ],
        [
         "programming"
        ],
        [
         "guide,"
        ],
        [
         "on"
        ],
        [
         "the"
        ],
        [
         "[project"
        ],
        [
         "web"
        ],
        [
         "page](http://spark.apache.org/documentation.html)"
        ],
        [
         "and"
        ],
        [
         "[project"
        ],
        [
         "wiki](https://cwiki.apache.org/confluence/display/SPARK)."
        ],
        [
         "This"
        ],
        [
         "README"
        ],
        [
         "file"
        ],
        [
         "only"
        ],
        [
         "contains"
        ],
        [
         "basic"
        ],
        [
         "setup"
        ],
        [
         "instructions."
        ],
        [
         "##"
        ],
        [
         "Building"
        ],
        [
         "Spark"
        ],
        [
         "Spark"
        ],
        [
         "is"
        ],
        [
         "built"
        ],
        [
         "using"
        ],
        [
         "[Apache"
        ],
        [
         "Maven](http://maven.apache.org/)."
        ],
        [
         "To"
        ],
        [
         "build"
        ],
        [
         "Spark"
        ],
        [
         "and"
        ],
        [
         "its"
        ],
        [
         "example"
        ],
        [
         "programs,"
        ],
        [
         "run:"
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         "build/mvn"
        ],
        [
         "-DskipTests"
        ],
        [
         "clean"
        ],
        [
         "package"
        ],
        [
         "(You"
        ],
        [
         "do"
        ],
        [
         "not"
        ],
        [
         "need"
        ],
        [
         "to"
        ],
        [
         "do"
        ],
        [
         "this"
        ],
        [
         "if"
        ],
        [
         "you"
        ],
        [
         "downloaded"
        ],
        [
         "a"
        ],
        [
         "pre-built"
        ],
        [
         "package.)"
        ],
        [
         "More"
        ],
        [
         "detailed"
        ],
        [
         "documentation"
        ],
        [
         "is"
        ],
        [
         "available"
        ],
        [
         "from"
        ],
        [
         "the"
        ],
        [
         "project"
        ],
        [
         "site,"
        ],
        [
         "at"
        ],
        [
         "[\"Building"
        ],
        [
         "Spark\"](http://spark.apache.org/docs/latest/building-spark.html)."
        ],
        [
         "##"
        ],
        [
         "Interactive"
        ],
        [
         "Scala"
        ],
        [
         "Shell"
        ],
        [
         "The"
        ],
        [
         "easiest"
        ],
        [
         "way"
        ],
        [
         "to"
        ],
        [
         "start"
        ],
        [
         "using"
        ],
        [
         "Spark"
        ],
        [
         "is"
        ],
        [
         "through"
        ],
        [
         "the"
        ],
        [
         "Scala"
        ],
        [
         "shell:"
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         "./bin/spark-shell"
        ],
        [
         "Try"
        ],
        [
         "the"
        ],
        [
         "following"
        ],
        [
         "command,"
        ],
        [
         "which"
        ],
        [
         "should"
        ],
        [
         "return"
        ],
        [
         "1000:"
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         "scala>"
        ],
        [
         "sc.parallelize(1"
        ],
        [
         "to"
        ],
        [
         "1000).count()"
        ],
        [
         "##"
        ],
        [
         "Interactive"
        ],
        [
         "Python"
        ],
        [
         "Shell"
        ],
        [
         "Alternatively,"
        ],
        [
         "if"
        ],
        [
         "you"
        ],
        [
         "prefer"
        ],
        [
         "Python,"
        ],
        [
         "you"
        ],
        [
         "can"
        ],
        [
         "use"
        ],
        [
         "the"
        ],
        [
         "Python"
        ],
        [
         "shell:"
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         "./bin/pyspark"
        ],
        [
         "And"
        ],
        [
         "run"
        ],
        [
         "the"
        ],
        [
         "following"
        ],
        [
         "command,"
        ],
        [
         "which"
        ],
        [
         "should"
        ],
        [
         "also"
        ],
        [
         "return"
        ],
        [
         "1000:"
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ">>>"
        ],
        [
         "sc.parallelize(range(1000)).count()"
        ],
        [
         "##"
        ],
        [
         "Example"
        ],
        [
         "Programs"
        ],
        [
         "Spark"
        ],
        [
         "also"
        ],
        [
         "comes"
        ],
        [
         "with"
        ],
        [
         "several"
        ],
        [
         "sample"
        ],
        [
         "programs"
        ],
        [
         "in"
        ],
        [
         "the"
        ],
        [
         "`examples`"
        ],
        [
         "directory."
        ],
        [
         "To"
        ],
        [
         "run"
        ],
        [
         "one"
        ],
        [
         "of"
        ],
        [
         "them,"
        ],
        [
         "use"
        ],
        [
         "`./bin/run-example"
        ],
        [
         "<class>"
        ],
        [
         "[params]`."
        ],
        [
         "For"
        ],
        [
         "example:"
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         "./bin/run-example"
        ],
        [
         "SparkPi"
        ],
        [
         "will"
        ],
        [
         "run"
        ],
        [
         "the"
        ],
        [
         "Pi"
        ],
        [
         "example"
        ],
        [
         "locally."
        ],
        [
         "You"
        ],
        [
         "can"
        ],
        [
         "set"
        ],
        [
         "the"
        ],
        [
         "MASTER"
        ],
        [
         "environment"
        ],
        [
         "variable"
        ],
        [
         "when"
        ],
        [
         "running"
        ],
        [
         "examples"
        ],
        [
         "to"
        ],
        [
         "submit"
        ],
        [
         "examples"
        ],
        [
         "to"
        ],
        [
         "a"
        ],
        [
         "cluster."
        ],
        [
         "This"
        ],
        [
         "can"
        ],
        [
         "be"
        ],
        [
         "a"
        ],
        [
         "mesos://"
        ],
        [
         "or"
        ],
        [
         "spark://"
        ],
        [
         "URL,"
        ],
        [
         "\"yarn\""
        ],
        [
         "to"
        ],
        [
         "run"
        ],
        [
         "on"
        ],
        [
         "YARN,"
        ],
        [
         "and"
        ],
        [
         "\"local\""
        ],
        [
         "to"
        ],
        [
         "run"
        ],
        [
         "locally"
        ],
        [
         "with"
        ],
        [
         "one"
        ],
        [
         "thread,"
        ],
        [
         "or"
        ],
        [
         "\"local[N]\""
        ],
        [
         "to"
        ],
        [
         "run"
        ],
        [
         "locally"
        ],
        [
         "with"
        ],
        [
         "N"
        ],
        [
         "threads."
        ],
        [
         "You"
        ],
        [
         "can"
        ],
        [
         "also"
        ],
        [
         "use"
        ],
        [
         "an"
        ],
        [
         "abbreviated"
        ],
        [
         "class"
        ],
        [
         "name"
        ],
        [
         "if"
        ],
        [
         "the"
        ],
        [
         "class"
        ],
        [
         "is"
        ],
        [
         "in"
        ],
        [
         "the"
        ],
        [
         "`examples`"
        ],
        [
         "package."
        ],
        [
         "For"
        ],
        [
         "instance:"
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         "MASTER=spark://host:7077"
        ],
        [
         "./bin/run-example"
        ],
        [
         "SparkPi"
        ],
        [
         "Many"
        ],
        [
         "of"
        ],
        [
         "the"
        ],
        [
         "example"
        ],
        [
         "programs"
        ],
        [
         "print"
        ],
        [
         "usage"
        ],
        [
         "help"
        ],
        [
         "if"
        ],
        [
         "no"
        ],
        [
         "params"
        ],
        [
         "are"
        ],
        [
         "given."
        ],
        [
         "##"
        ],
        [
         "Running"
        ],
        [
         "Tests"
        ],
        [
         "Testing"
        ],
        [
         "first"
        ],
        [
         "requires"
        ],
        [
         "[building"
        ],
        [
         "Spark](#building-spark)."
        ],
        [
         "Once"
        ],
        [
         "Spark"
        ],
        [
         "is"
        ],
        [
         "built,"
        ],
        [
         "tests"
        ],
        [
         "can"
        ],
        [
         "be"
        ],
        [
         "run"
        ],
        [
         "using:"
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         ""
        ],
        [
         "./dev/run-tests"
        ],
        [
         "Please"
        ],
        [
         "see"
        ],
        [
         "the"
        ],
        [
         "guidance"
        ],
        [
         "on"
        ],
        [
         "how"
        ],
        [
         "to"
        ],
        [
         "[run"
        ],
        [
         "tests"
        ],
        [
         "for"
        ],
        [
         "a"
        ],
        [
         "module,"
        ],
        [
         "or"
        ],
        [
         "individual"
        ],
        [
         "tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools)."
        ],
        [
         "##"
        ],
        [
         "A"
        ],
        [
         "Note"
        ],
        [
         "About"
        ],
        [
         "Hadoop"
        ],
        [
         "Versions"
        ],
        [
         "Spark"
        ],
        [
         "uses"
        ],
        [
         "the"
        ],
        [
         "Hadoop"
        ],
        [
         "core"
        ],
        [
         "library"
        ],
        [
         "to"
        ],
        [
         "talk"
        ],
        [
         "to"
        ],
        [
         "HDFS"
        ],
        [
         "and"
        ],
        [
         "other"
        ],
        [
         "Hadoop-supported"
        ],
        [
         "storage"
        ],
        [
         "systems."
        ],
        [
         "Because"
        ],
        [
         "the"
        ],
        [
         "protocols"
        ],
        [
         "have"
        ],
        [
         "changed"
        ],
        [
         "in"
        ],
        [
         "different"
        ],
        [
         "versions"
        ],
        [
         "of"
        ],
        [
         "Hadoop,"
        ],
        [
         "you"
        ],
        [
         "must"
        ],
        [
         "build"
        ],
        [
         "Spark"
        ],
        [
         "against"
        ],
        [
         "the"
        ],
        [
         "same"
        ],
        [
         "version"
        ],
        [
         "that"
        ],
        [
         "your"
        ],
        [
         "cluster"
        ],
        [
         "runs."
        ],
        [
         "Please"
        ],
        [
         "refer"
        ],
        [
         "to"
        ],
        [
         "the"
        ],
        [
         "build"
        ],
        [
         "documentation"
        ],
        [
         "at"
        ],
        [
         "[\"Specifying"
        ],
        [
         "the"
        ],
        [
         "Hadoop"
        ],
        [
         "Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)"
        ],
        [
         "for"
        ],
        [
         "detailed"
        ],
        [
         "guidance"
        ],
        [
         "on"
        ],
        [
         "building"
        ],
        [
         "for"
        ],
        [
         "a"
        ],
        [
         "particular"
        ],
        [
         "distribution"
        ],
        [
         "of"
        ],
        [
         "Hadoop,"
        ],
        [
         "including"
        ],
        [
         "building"
        ],
        [
         "for"
        ],
        [
         "particular"
        ],
        [
         "Hive"
        ],
        [
         "and"
        ],
        [
         "Hive"
        ],
        [
         "Thriftserver"
        ],
        [
         "distributions."
        ],
        [
         "##"
        ],
        [
         "Configuration"
        ],
        [
         "Please"
        ],
        [
         "refer"
        ],
        [
         "to"
        ],
        [
         "the"
        ],
        [
         "[Configuration"
        ],
        [
         "Guide](http://spark.apache.org/docs/latest/configuration.html)"
        ],
        [
         "in"
        ],
        [
         "the"
        ],
        [
         "online"
        ],
        [
         "documentation"
        ],
        [
         "for"
        ],
        [
         "an"
        ],
        [
         "overview"
        ],
        [
         "on"
        ],
        [
         "how"
        ],
        [
         "to"
        ],
        [
         "configure"
        ],
        [
         "Spark."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "word",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7612ecc-caae-43fe-948c-9c8257c4a95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard Transformations\n",
    "# Projection (select)\n",
    "# Filtering (filter or where)\n",
    "# Aggregations by key (groupBy)\n",
    "# Joining multiple dataframes (join)\n",
    "# Sorting the data (orderBy)\n",
    "# Windowing or Analytical Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3329c07f-c115-454c-960b-7e95d890589a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792a8b43-9d3b-4345-850d-2b3ee1fcf22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>word</th><th>word_count</th></tr></thead><tbody><tr><td></td><td>32</td></tr><tr><td>the</td><td>21</td></tr><tr><td>to</td><td>14</td></tr><tr><td>Spark</td><td>13</td></tr><tr><td>for</td><td>11</td></tr><tr><td>and</td><td>10</td></tr><tr><td>##</td><td>8</td></tr><tr><td>a</td><td>8</td></tr><tr><td>run</td><td>7</td></tr><tr><td>can</td><td>6</td></tr><tr><td>is</td><td>6</td></tr><tr><td>in</td><td>5</td></tr><tr><td>on</td><td>5</td></tr><tr><td>of</td><td>5</td></tr><tr><td>you</td><td>4</td></tr><tr><td>if</td><td>4</td></tr><tr><td>also</td><td>4</td></tr><tr><td>documentation</td><td>3</td></tr><tr><td>example</td><td>3</td></tr><tr><td>with</td><td>3</td></tr><tr><td>use</td><td>3</td></tr><tr><td>You</td><td>3</td></tr><tr><td>Please</td><td>3</td></tr><tr><td>build</td><td>3</td></tr><tr><td>an</td><td>3</td></tr><tr><td>including</td><td>3</td></tr><tr><td>or</td><td>3</td></tr><tr><td>Hadoop</td><td>3</td></tr><tr><td>command,</td><td>2</td></tr><tr><td>set</td><td>2</td></tr><tr><td>programs</td><td>2</td></tr><tr><td>tests</td><td>2</td></tr><tr><td>particular</td><td>2</td></tr><tr><td>using</td><td>2</td></tr><tr><td>guidance</td><td>2</td></tr><tr><td>shell:</td><td>2</td></tr><tr><td>Interactive</td><td>2</td></tr><tr><td>how</td><td>2</td></tr><tr><td>Hive</td><td>2</td></tr><tr><td>one</td><td>2</td></tr><tr><td>building</td><td>2</td></tr><tr><td>be</td><td>2</td></tr><tr><td>locally</td><td>2</td></tr><tr><td>should</td><td>2</td></tr><tr><td>[project</td><td>2</td></tr><tr><td>`examples`</td><td>2</td></tr><tr><td>general</td><td>2</td></tr><tr><td>1000:</td><td>2</td></tr><tr><td>detailed</td><td>2</td></tr><tr><td>following</td><td>2</td></tr><tr><td>SparkPi</td><td>2</td></tr><tr><td>refer</td><td>2</td></tr><tr><td>./bin/run-example</td><td>2</td></tr><tr><td>It</td><td>2</td></tr><tr><td>To</td><td>2</td></tr><tr><td>return</td><td>2</td></tr><tr><td>cluster</td><td>2</td></tr><tr><td>do</td><td>2</td></tr><tr><td>Scala</td><td>2</td></tr><tr><td>class</td><td>2</td></tr><tr><td>Hadoop,</td><td>2</td></tr><tr><td>SQL</td><td>2</td></tr><tr><td>Python,</td><td>2</td></tr><tr><td>examples</td><td>2</td></tr><tr><td>Python</td><td>2</td></tr><tr><td>at</td><td>2</td></tr><tr><td>that</td><td>2</td></tr><tr><td>For</td><td>2</td></tr><tr><td>This</td><td>2</td></tr><tr><td>Shell</td><td>2</td></tr><tr><td>supports</td><td>2</td></tr><tr><td>which</td><td>2</td></tr><tr><td>online</td><td>1</td></tr><tr><td>graphs</td><td>1</td></tr><tr><td>[\"Building</td><td>1</td></tr><tr><td>abbreviated</td><td>1</td></tr><tr><td>overview</td><td>1</td></tr><tr><td>rich</td><td>1</td></tr><tr><td>-DskipTests</td><td>1</td></tr><tr><td>name</td><td>1</td></tr><tr><td>[\"Specifying</td><td>1</td></tr><tr><td>stream</td><td>1</td></tr><tr><td>run:</td><td>1</td></tr><tr><td>not</td><td>1</td></tr><tr><td>./dev/run-tests</td><td>1</td></tr><tr><td>will</td><td>1</td></tr><tr><td>[run</td><td>1</td></tr><tr><td>Alternatively,</td><td>1</td></tr><tr><td>must</td><td>1</td></tr><tr><td>MLlib</td><td>1</td></tr><tr><td>DataFrames,</td><td>1</td></tr><tr><td>variable</td><td>1</td></tr><tr><td>Note</td><td>1</td></tr><tr><td>core</td><td>1</td></tr><tr><td>protocols</td><td>1</td></tr><tr><td>site,</td><td>1</td></tr><tr><td>systems.</td><td>1</td></tr><tr><td>[building</td><td>1</td></tr><tr><td>configure</td><td>1</td></tr><tr><td>README</td><td>1</td></tr><tr><td>[Configuration</td><td>1</td></tr><tr><td>system</td><td>1</td></tr><tr><td>provides</td><td>1</td></tr><tr><td>Hadoop-supported</td><td>1</td></tr><tr><td>pre-built</td><td>1</td></tr><tr><td>directory.</td><td>1</td></tr><tr><td>wiki](https://cwiki.apache.org/confluence/display/SPARK).</td><td>1</td></tr><tr><td>Example</td><td>1</td></tr><tr><td>MASTER</td><td>1</td></tr><tr><td>library</td><td>1</td></tr><tr><td>Spark.</td><td>1</td></tr><tr><td>contains</td><td>1</td></tr><tr><td>Configuration</td><td>1</td></tr><tr><td>programming</td><td>1</td></tr><tr><td>downloaded</td><td>1</td></tr><tr><td>1000).count()</td><td>1</td></tr><tr><td>comes</td><td>1</td></tr><tr><td>machine</td><td>1</td></tr><tr><td>Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)</td><td>1</td></tr><tr><td>params</td><td>1</td></tr><tr><td>Guide](http://spark.apache.org/docs/latest/configuration.html)</td><td>1</td></tr><tr><td>given.</td><td>1</td></tr><tr><td>same</td><td>1</td></tr><tr><td>page](http://spark.apache.org/documentation.html)</td><td>1</td></tr><tr><td>Programs</td><td>1</td></tr><tr><td>using:</td><td>1</td></tr><tr><td>fast</td><td>1</td></tr><tr><td>[Apache</td><td>1</td></tr><tr><td>your</td><td>1</td></tr><tr><td>optimized</td><td>1</td></tr><tr><td>R,</td><td>1</td></tr><tr><td>graph</td><td>1</td></tr><tr><td>package</td><td>1</td></tr><tr><td>project</td><td>1</td></tr><tr><td>versions</td><td>1</td></tr><tr><td>Spark](#building-spark).</td><td>1</td></tr><tr><td>other</td><td>1</td></tr><tr><td>learning,</td><td>1</td></tr><tr><td>when</td><td>1</td></tr><tr><td>submit</td><td>1</td></tr><tr><td>Apache</td><td>1</td></tr><tr><td>About</td><td>1</td></tr><tr><td>scala></td><td>1</td></tr><tr><td>print</td><td>1</td></tr><tr><td>different</td><td>1</td></tr><tr><td>data</td><td>1</td></tr><tr><td>Tests</td><td>1</td></tr><tr><td>Versions</td><td>1</td></tr><tr><td>Data.</td><td>1</td></tr><tr><td>processing.</td><td>1</td></tr><tr><td>its</td><td>1</td></tr><tr><td>basic</td><td>1</td></tr><tr><td>latest</td><td>1</td></tr><tr><td>only</td><td>1</td></tr><tr><td><class></td><td>1</td></tr><tr><td>have</td><td>1</td></tr><tr><td>runs.</td><td>1</td></tr><tr><td>YARN,</td><td>1</td></tr><tr><td>\"local\"</td><td>1</td></tr><tr><td>processing,</td><td>1</td></tr><tr><td>built</td><td>1</td></tr><tr><td>Pi</td><td>1</td></tr><tr><td>thread,</td><td>1</td></tr><tr><td>A</td><td>1</td></tr><tr><td>APIs</td><td>1</td></tr><tr><td>Scala,</td><td>1</td></tr><tr><td>file</td><td>1</td></tr><tr><td>computation</td><td>1</td></tr><tr><td>Once</td><td>1</td></tr><tr><td>find</td><td>1</td></tr><tr><td>sc.parallelize(1</td><td>1</td></tr><tr><td>uses</td><td>1</td></tr><tr><td>N</td><td>1</td></tr><tr><td>programs,</td><td>1</td></tr><tr><td>\"yarn\"</td><td>1</td></tr><tr><td>see</td><td>1</td></tr><tr><td>./bin/pyspark</td><td>1</td></tr><tr><td>computing</td><td>1</td></tr><tr><td>Java,</td><td>1</td></tr><tr><td>from</td><td>1</td></tr><tr><td>Because</td><td>1</td></tr><tr><td>Streaming</td><td>1</td></tr><tr><td>More</td><td>1</td></tr><tr><td>analysis.</td><td>1</td></tr><tr><td>Maven](http://maven.apache.org/).</td><td>1</td></tr><tr><td>cluster.</td><td>1</td></tr><tr><td>Running</td><td>1</td></tr><tr><td>talk</td><td>1</td></tr><tr><td>distributions.</td><td>1</td></tr><tr><td>guide,</td><td>1</td></tr><tr><td>\"local[N]\"</td><td>1</td></tr><tr><td>Try</td><td>1</td></tr><tr><td>setup</td><td>1</td></tr><tr><td>need</td><td>1</td></tr><tr><td>spark://</td><td>1</td></tr><tr><td>Thriftserver</td><td>1</td></tr><tr><td>are</td><td>1</td></tr><tr><td>requires</td><td>1</td></tr><tr><td>package.</td><td>1</td></tr><tr><td>clean</td><td>1</td></tr><tr><td>sc.parallelize(range(1000)).count()</td><td>1</td></tr><tr><td>high-level</td><td>1</td></tr><tr><td>against</td><td>1</td></tr><tr><td>through</td><td>1</td></tr><tr><td>package.)</td><td>1</td></tr><tr><td>easiest</td><td>1</td></tr><tr><td>no</td><td>1</td></tr><tr><td>Testing</td><td>1</td></tr><tr><td>several</td><td>1</td></tr><tr><td>help</td><td>1</td></tr><tr><td>The</td><td>1</td></tr><tr><td>sample</td><td>1</td></tr><tr><td>MASTER=spark://host:7077</td><td>1</td></tr><tr><td>Big</td><td>1</td></tr><tr><td>#</td><td>1</td></tr><tr><td>Online</td><td>1</td></tr><tr><td>usage</td><td>1</td></tr><tr><td>Spark\"](http://spark.apache.org/docs/latest/building-spark.html).</td><td>1</td></tr><tr><td>way</td><td>1</td></tr><tr><td>prefer</td><td>1</td></tr><tr><td>build/mvn</td><td>1</td></tr><tr><td>running</td><td>1</td></tr><tr><td>web</td><td>1</td></tr><tr><td>locally.</td><td>1</td></tr><tr><td>URL,</td><td>1</td></tr><tr><td>higher-level</td><td>1</td></tr><tr><td>tools</td><td>1</td></tr><tr><td>available</td><td>1</td></tr><tr><td>Documentation</td><td>1</td></tr><tr><td>this</td><td>1</td></tr><tr><td>tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).</td><td>1</td></tr><tr><td>(You</td><td>1</td></tr><tr><td>>>></td><td>1</td></tr><tr><td><http://spark.apache.org/></td><td>1</td></tr><tr><td>environment</td><td>1</td></tr><tr><td>built,</td><td>1</td></tr><tr><td>module,</td><td>1</td></tr><tr><td>them,</td><td>1</td></tr><tr><td>`./bin/run-example</td><td>1</td></tr><tr><td>instance:</td><td>1</td></tr><tr><td>first</td><td>1</td></tr><tr><td>documentation,</td><td>1</td></tr><tr><td>[params]`.</td><td>1</td></tr><tr><td>mesos://</td><td>1</td></tr><tr><td>engine</td><td>1</td></tr><tr><td>GraphX</td><td>1</td></tr><tr><td>example:</td><td>1</td></tr><tr><td>HDFS</td><td>1</td></tr><tr><td>individual</td><td>1</td></tr><tr><td>changed</td><td>1</td></tr><tr><td>./bin/spark-shell</td><td>1</td></tr><tr><td>threads.</td><td>1</td></tr><tr><td>storage</td><td>1</td></tr><tr><td>version</td><td>1</td></tr><tr><td>instructions.</td><td>1</td></tr><tr><td>Building</td><td>1</td></tr><tr><td>start</td><td>1</td></tr><tr><td>Many</td><td>1</td></tr><tr><td>And</td><td>1</td></tr><tr><td>distribution</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "",
         32
        ],
        [
         "the",
         21
        ],
        [
         "to",
         14
        ],
        [
         "Spark",
         13
        ],
        [
         "for",
         11
        ],
        [
         "and",
         10
        ],
        [
         "##",
         8
        ],
        [
         "a",
         8
        ],
        [
         "run",
         7
        ],
        [
         "can",
         6
        ],
        [
         "is",
         6
        ],
        [
         "in",
         5
        ],
        [
         "on",
         5
        ],
        [
         "of",
         5
        ],
        [
         "you",
         4
        ],
        [
         "if",
         4
        ],
        [
         "also",
         4
        ],
        [
         "documentation",
         3
        ],
        [
         "example",
         3
        ],
        [
         "with",
         3
        ],
        [
         "use",
         3
        ],
        [
         "You",
         3
        ],
        [
         "Please",
         3
        ],
        [
         "build",
         3
        ],
        [
         "an",
         3
        ],
        [
         "including",
         3
        ],
        [
         "or",
         3
        ],
        [
         "Hadoop",
         3
        ],
        [
         "command,",
         2
        ],
        [
         "set",
         2
        ],
        [
         "programs",
         2
        ],
        [
         "tests",
         2
        ],
        [
         "particular",
         2
        ],
        [
         "using",
         2
        ],
        [
         "guidance",
         2
        ],
        [
         "shell:",
         2
        ],
        [
         "Interactive",
         2
        ],
        [
         "how",
         2
        ],
        [
         "Hive",
         2
        ],
        [
         "one",
         2
        ],
        [
         "building",
         2
        ],
        [
         "be",
         2
        ],
        [
         "locally",
         2
        ],
        [
         "should",
         2
        ],
        [
         "[project",
         2
        ],
        [
         "`examples`",
         2
        ],
        [
         "general",
         2
        ],
        [
         "1000:",
         2
        ],
        [
         "detailed",
         2
        ],
        [
         "following",
         2
        ],
        [
         "SparkPi",
         2
        ],
        [
         "refer",
         2
        ],
        [
         "./bin/run-example",
         2
        ],
        [
         "It",
         2
        ],
        [
         "To",
         2
        ],
        [
         "return",
         2
        ],
        [
         "cluster",
         2
        ],
        [
         "do",
         2
        ],
        [
         "Scala",
         2
        ],
        [
         "class",
         2
        ],
        [
         "Hadoop,",
         2
        ],
        [
         "SQL",
         2
        ],
        [
         "Python,",
         2
        ],
        [
         "examples",
         2
        ],
        [
         "Python",
         2
        ],
        [
         "at",
         2
        ],
        [
         "that",
         2
        ],
        [
         "For",
         2
        ],
        [
         "This",
         2
        ],
        [
         "Shell",
         2
        ],
        [
         "supports",
         2
        ],
        [
         "which",
         2
        ],
        [
         "online",
         1
        ],
        [
         "graphs",
         1
        ],
        [
         "[\"Building",
         1
        ],
        [
         "abbreviated",
         1
        ],
        [
         "overview",
         1
        ],
        [
         "rich",
         1
        ],
        [
         "-DskipTests",
         1
        ],
        [
         "name",
         1
        ],
        [
         "[\"Specifying",
         1
        ],
        [
         "stream",
         1
        ],
        [
         "run:",
         1
        ],
        [
         "not",
         1
        ],
        [
         "./dev/run-tests",
         1
        ],
        [
         "will",
         1
        ],
        [
         "[run",
         1
        ],
        [
         "Alternatively,",
         1
        ],
        [
         "must",
         1
        ],
        [
         "MLlib",
         1
        ],
        [
         "DataFrames,",
         1
        ],
        [
         "variable",
         1
        ],
        [
         "Note",
         1
        ],
        [
         "core",
         1
        ],
        [
         "protocols",
         1
        ],
        [
         "site,",
         1
        ],
        [
         "systems.",
         1
        ],
        [
         "[building",
         1
        ],
        [
         "configure",
         1
        ],
        [
         "README",
         1
        ],
        [
         "[Configuration",
         1
        ],
        [
         "system",
         1
        ],
        [
         "provides",
         1
        ],
        [
         "Hadoop-supported",
         1
        ],
        [
         "pre-built",
         1
        ],
        [
         "directory.",
         1
        ],
        [
         "wiki](https://cwiki.apache.org/confluence/display/SPARK).",
         1
        ],
        [
         "Example",
         1
        ],
        [
         "MASTER",
         1
        ],
        [
         "library",
         1
        ],
        [
         "Spark.",
         1
        ],
        [
         "contains",
         1
        ],
        [
         "Configuration",
         1
        ],
        [
         "programming",
         1
        ],
        [
         "downloaded",
         1
        ],
        [
         "1000).count()",
         1
        ],
        [
         "comes",
         1
        ],
        [
         "machine",
         1
        ],
        [
         "Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)",
         1
        ],
        [
         "params",
         1
        ],
        [
         "Guide](http://spark.apache.org/docs/latest/configuration.html)",
         1
        ],
        [
         "given.",
         1
        ],
        [
         "same",
         1
        ],
        [
         "page](http://spark.apache.org/documentation.html)",
         1
        ],
        [
         "Programs",
         1
        ],
        [
         "using:",
         1
        ],
        [
         "fast",
         1
        ],
        [
         "[Apache",
         1
        ],
        [
         "your",
         1
        ],
        [
         "optimized",
         1
        ],
        [
         "R,",
         1
        ],
        [
         "graph",
         1
        ],
        [
         "package",
         1
        ],
        [
         "project",
         1
        ],
        [
         "versions",
         1
        ],
        [
         "Spark](#building-spark).",
         1
        ],
        [
         "other",
         1
        ],
        [
         "learning,",
         1
        ],
        [
         "when",
         1
        ],
        [
         "submit",
         1
        ],
        [
         "Apache",
         1
        ],
        [
         "About",
         1
        ],
        [
         "scala>",
         1
        ],
        [
         "print",
         1
        ],
        [
         "different",
         1
        ],
        [
         "data",
         1
        ],
        [
         "Tests",
         1
        ],
        [
         "Versions",
         1
        ],
        [
         "Data.",
         1
        ],
        [
         "processing.",
         1
        ],
        [
         "its",
         1
        ],
        [
         "basic",
         1
        ],
        [
         "latest",
         1
        ],
        [
         "only",
         1
        ],
        [
         "<class>",
         1
        ],
        [
         "have",
         1
        ],
        [
         "runs.",
         1
        ],
        [
         "YARN,",
         1
        ],
        [
         "\"local\"",
         1
        ],
        [
         "processing,",
         1
        ],
        [
         "built",
         1
        ],
        [
         "Pi",
         1
        ],
        [
         "thread,",
         1
        ],
        [
         "A",
         1
        ],
        [
         "APIs",
         1
        ],
        [
         "Scala,",
         1
        ],
        [
         "file",
         1
        ],
        [
         "computation",
         1
        ],
        [
         "Once",
         1
        ],
        [
         "find",
         1
        ],
        [
         "sc.parallelize(1",
         1
        ],
        [
         "uses",
         1
        ],
        [
         "N",
         1
        ],
        [
         "programs,",
         1
        ],
        [
         "\"yarn\"",
         1
        ],
        [
         "see",
         1
        ],
        [
         "./bin/pyspark",
         1
        ],
        [
         "computing",
         1
        ],
        [
         "Java,",
         1
        ],
        [
         "from",
         1
        ],
        [
         "Because",
         1
        ],
        [
         "Streaming",
         1
        ],
        [
         "More",
         1
        ],
        [
         "analysis.",
         1
        ],
        [
         "Maven](http://maven.apache.org/).",
         1
        ],
        [
         "cluster.",
         1
        ],
        [
         "Running",
         1
        ],
        [
         "talk",
         1
        ],
        [
         "distributions.",
         1
        ],
        [
         "guide,",
         1
        ],
        [
         "\"local[N]\"",
         1
        ],
        [
         "Try",
         1
        ],
        [
         "setup",
         1
        ],
        [
         "need",
         1
        ],
        [
         "spark://",
         1
        ],
        [
         "Thriftserver",
         1
        ],
        [
         "are",
         1
        ],
        [
         "requires",
         1
        ],
        [
         "package.",
         1
        ],
        [
         "clean",
         1
        ],
        [
         "sc.parallelize(range(1000)).count()",
         1
        ],
        [
         "high-level",
         1
        ],
        [
         "against",
         1
        ],
        [
         "through",
         1
        ],
        [
         "package.)",
         1
        ],
        [
         "easiest",
         1
        ],
        [
         "no",
         1
        ],
        [
         "Testing",
         1
        ],
        [
         "several",
         1
        ],
        [
         "help",
         1
        ],
        [
         "The",
         1
        ],
        [
         "sample",
         1
        ],
        [
         "MASTER=spark://host:7077",
         1
        ],
        [
         "Big",
         1
        ],
        [
         "#",
         1
        ],
        [
         "Online",
         1
        ],
        [
         "usage",
         1
        ],
        [
         "Spark\"](http://spark.apache.org/docs/latest/building-spark.html).",
         1
        ],
        [
         "way",
         1
        ],
        [
         "prefer",
         1
        ],
        [
         "build/mvn",
         1
        ],
        [
         "running",
         1
        ],
        [
         "web",
         1
        ],
        [
         "locally.",
         1
        ],
        [
         "URL,",
         1
        ],
        [
         "higher-level",
         1
        ],
        [
         "tools",
         1
        ],
        [
         "available",
         1
        ],
        [
         "Documentation",
         1
        ],
        [
         "this",
         1
        ],
        [
         "tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).",
         1
        ],
        [
         "(You",
         1
        ],
        [
         ">>>",
         1
        ],
        [
         "<http://spark.apache.org/>",
         1
        ],
        [
         "environment",
         1
        ],
        [
         "built,",
         1
        ],
        [
         "module,",
         1
        ],
        [
         "them,",
         1
        ],
        [
         "`./bin/run-example",
         1
        ],
        [
         "instance:",
         1
        ],
        [
         "first",
         1
        ],
        [
         "documentation,",
         1
        ],
        [
         "[params]`.",
         1
        ],
        [
         "mesos://",
         1
        ],
        [
         "engine",
         1
        ],
        [
         "GraphX",
         1
        ],
        [
         "example:",
         1
        ],
        [
         "HDFS",
         1
        ],
        [
         "individual",
         1
        ],
        [
         "changed",
         1
        ],
        [
         "./bin/spark-shell",
         1
        ],
        [
         "threads.",
         1
        ],
        [
         "storage",
         1
        ],
        [
         "version",
         1
        ],
        [
         "instructions.",
         1
        ],
        [
         "Building",
         1
        ],
        [
         "start",
         1
        ],
        [
         "Many",
         1
        ],
        [
         "And",
         1
        ],
        [
         "distribution",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "word",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "word_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words. \\\n",
    "    groupBy('word'). \\\n",
    "    agg(count(\"*\").alias('word_count')). \\\n",
    "    orderBy(col('word_count').desc()). \\\n",
    "    display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06482fc8-866e-41a3-8bd9-bdce468bdf75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "word_count = words. \\\n",
    "    groupBy('word'). \\\n",
    "    agg(count(\"*\").alias('word_count')). \\\n",
    "    orderBy(col('word_count').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d646afc9-819e-4a2d-8b1c-99b623af1e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>word</th><th>word_count</th></tr></thead><tbody><tr><td></td><td>32</td></tr><tr><td>the</td><td>21</td></tr><tr><td>to</td><td>14</td></tr><tr><td>Spark</td><td>13</td></tr><tr><td>for</td><td>11</td></tr><tr><td>and</td><td>10</td></tr><tr><td>##</td><td>8</td></tr><tr><td>a</td><td>8</td></tr><tr><td>run</td><td>7</td></tr><tr><td>can</td><td>6</td></tr><tr><td>is</td><td>6</td></tr><tr><td>in</td><td>5</td></tr><tr><td>on</td><td>5</td></tr><tr><td>of</td><td>5</td></tr><tr><td>you</td><td>4</td></tr><tr><td>if</td><td>4</td></tr><tr><td>also</td><td>4</td></tr><tr><td>documentation</td><td>3</td></tr><tr><td>example</td><td>3</td></tr><tr><td>with</td><td>3</td></tr><tr><td>use</td><td>3</td></tr><tr><td>You</td><td>3</td></tr><tr><td>Please</td><td>3</td></tr><tr><td>build</td><td>3</td></tr><tr><td>an</td><td>3</td></tr><tr><td>including</td><td>3</td></tr><tr><td>or</td><td>3</td></tr><tr><td>Hadoop</td><td>3</td></tr><tr><td>command,</td><td>2</td></tr><tr><td>set</td><td>2</td></tr><tr><td>programs</td><td>2</td></tr><tr><td>tests</td><td>2</td></tr><tr><td>particular</td><td>2</td></tr><tr><td>using</td><td>2</td></tr><tr><td>guidance</td><td>2</td></tr><tr><td>shell:</td><td>2</td></tr><tr><td>Interactive</td><td>2</td></tr><tr><td>how</td><td>2</td></tr><tr><td>Hive</td><td>2</td></tr><tr><td>one</td><td>2</td></tr><tr><td>building</td><td>2</td></tr><tr><td>be</td><td>2</td></tr><tr><td>locally</td><td>2</td></tr><tr><td>should</td><td>2</td></tr><tr><td>[project</td><td>2</td></tr><tr><td>`examples`</td><td>2</td></tr><tr><td>general</td><td>2</td></tr><tr><td>1000:</td><td>2</td></tr><tr><td>detailed</td><td>2</td></tr><tr><td>following</td><td>2</td></tr><tr><td>SparkPi</td><td>2</td></tr><tr><td>refer</td><td>2</td></tr><tr><td>./bin/run-example</td><td>2</td></tr><tr><td>It</td><td>2</td></tr><tr><td>To</td><td>2</td></tr><tr><td>return</td><td>2</td></tr><tr><td>cluster</td><td>2</td></tr><tr><td>do</td><td>2</td></tr><tr><td>Scala</td><td>2</td></tr><tr><td>class</td><td>2</td></tr><tr><td>Hadoop,</td><td>2</td></tr><tr><td>SQL</td><td>2</td></tr><tr><td>Python,</td><td>2</td></tr><tr><td>examples</td><td>2</td></tr><tr><td>Python</td><td>2</td></tr><tr><td>at</td><td>2</td></tr><tr><td>that</td><td>2</td></tr><tr><td>For</td><td>2</td></tr><tr><td>This</td><td>2</td></tr><tr><td>Shell</td><td>2</td></tr><tr><td>supports</td><td>2</td></tr><tr><td>which</td><td>2</td></tr><tr><td>online</td><td>1</td></tr><tr><td>graphs</td><td>1</td></tr><tr><td>[\"Building</td><td>1</td></tr><tr><td>abbreviated</td><td>1</td></tr><tr><td>overview</td><td>1</td></tr><tr><td>rich</td><td>1</td></tr><tr><td>-DskipTests</td><td>1</td></tr><tr><td>name</td><td>1</td></tr><tr><td>[\"Specifying</td><td>1</td></tr><tr><td>stream</td><td>1</td></tr><tr><td>run:</td><td>1</td></tr><tr><td>not</td><td>1</td></tr><tr><td>./dev/run-tests</td><td>1</td></tr><tr><td>will</td><td>1</td></tr><tr><td>[run</td><td>1</td></tr><tr><td>Alternatively,</td><td>1</td></tr><tr><td>must</td><td>1</td></tr><tr><td>MLlib</td><td>1</td></tr><tr><td>DataFrames,</td><td>1</td></tr><tr><td>variable</td><td>1</td></tr><tr><td>Note</td><td>1</td></tr><tr><td>core</td><td>1</td></tr><tr><td>protocols</td><td>1</td></tr><tr><td>site,</td><td>1</td></tr><tr><td>systems.</td><td>1</td></tr><tr><td>[building</td><td>1</td></tr><tr><td>configure</td><td>1</td></tr><tr><td>README</td><td>1</td></tr><tr><td>[Configuration</td><td>1</td></tr><tr><td>system</td><td>1</td></tr><tr><td>provides</td><td>1</td></tr><tr><td>Hadoop-supported</td><td>1</td></tr><tr><td>pre-built</td><td>1</td></tr><tr><td>directory.</td><td>1</td></tr><tr><td>wiki](https://cwiki.apache.org/confluence/display/SPARK).</td><td>1</td></tr><tr><td>Example</td><td>1</td></tr><tr><td>MASTER</td><td>1</td></tr><tr><td>library</td><td>1</td></tr><tr><td>Spark.</td><td>1</td></tr><tr><td>contains</td><td>1</td></tr><tr><td>Configuration</td><td>1</td></tr><tr><td>programming</td><td>1</td></tr><tr><td>downloaded</td><td>1</td></tr><tr><td>1000).count()</td><td>1</td></tr><tr><td>comes</td><td>1</td></tr><tr><td>machine</td><td>1</td></tr><tr><td>Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)</td><td>1</td></tr><tr><td>params</td><td>1</td></tr><tr><td>Guide](http://spark.apache.org/docs/latest/configuration.html)</td><td>1</td></tr><tr><td>given.</td><td>1</td></tr><tr><td>same</td><td>1</td></tr><tr><td>page](http://spark.apache.org/documentation.html)</td><td>1</td></tr><tr><td>Programs</td><td>1</td></tr><tr><td>using:</td><td>1</td></tr><tr><td>fast</td><td>1</td></tr><tr><td>[Apache</td><td>1</td></tr><tr><td>your</td><td>1</td></tr><tr><td>optimized</td><td>1</td></tr><tr><td>R,</td><td>1</td></tr><tr><td>graph</td><td>1</td></tr><tr><td>package</td><td>1</td></tr><tr><td>project</td><td>1</td></tr><tr><td>versions</td><td>1</td></tr><tr><td>Spark](#building-spark).</td><td>1</td></tr><tr><td>other</td><td>1</td></tr><tr><td>learning,</td><td>1</td></tr><tr><td>when</td><td>1</td></tr><tr><td>submit</td><td>1</td></tr><tr><td>Apache</td><td>1</td></tr><tr><td>About</td><td>1</td></tr><tr><td>scala></td><td>1</td></tr><tr><td>print</td><td>1</td></tr><tr><td>different</td><td>1</td></tr><tr><td>data</td><td>1</td></tr><tr><td>Tests</td><td>1</td></tr><tr><td>Versions</td><td>1</td></tr><tr><td>Data.</td><td>1</td></tr><tr><td>processing.</td><td>1</td></tr><tr><td>its</td><td>1</td></tr><tr><td>basic</td><td>1</td></tr><tr><td>latest</td><td>1</td></tr><tr><td>only</td><td>1</td></tr><tr><td><class></td><td>1</td></tr><tr><td>have</td><td>1</td></tr><tr><td>runs.</td><td>1</td></tr><tr><td>YARN,</td><td>1</td></tr><tr><td>\"local\"</td><td>1</td></tr><tr><td>processing,</td><td>1</td></tr><tr><td>built</td><td>1</td></tr><tr><td>Pi</td><td>1</td></tr><tr><td>thread,</td><td>1</td></tr><tr><td>A</td><td>1</td></tr><tr><td>APIs</td><td>1</td></tr><tr><td>Scala,</td><td>1</td></tr><tr><td>file</td><td>1</td></tr><tr><td>computation</td><td>1</td></tr><tr><td>Once</td><td>1</td></tr><tr><td>find</td><td>1</td></tr><tr><td>sc.parallelize(1</td><td>1</td></tr><tr><td>uses</td><td>1</td></tr><tr><td>N</td><td>1</td></tr><tr><td>programs,</td><td>1</td></tr><tr><td>\"yarn\"</td><td>1</td></tr><tr><td>see</td><td>1</td></tr><tr><td>./bin/pyspark</td><td>1</td></tr><tr><td>computing</td><td>1</td></tr><tr><td>Java,</td><td>1</td></tr><tr><td>from</td><td>1</td></tr><tr><td>Because</td><td>1</td></tr><tr><td>Streaming</td><td>1</td></tr><tr><td>More</td><td>1</td></tr><tr><td>analysis.</td><td>1</td></tr><tr><td>Maven](http://maven.apache.org/).</td><td>1</td></tr><tr><td>cluster.</td><td>1</td></tr><tr><td>Running</td><td>1</td></tr><tr><td>talk</td><td>1</td></tr><tr><td>distributions.</td><td>1</td></tr><tr><td>guide,</td><td>1</td></tr><tr><td>\"local[N]\"</td><td>1</td></tr><tr><td>Try</td><td>1</td></tr><tr><td>setup</td><td>1</td></tr><tr><td>need</td><td>1</td></tr><tr><td>spark://</td><td>1</td></tr><tr><td>Thriftserver</td><td>1</td></tr><tr><td>are</td><td>1</td></tr><tr><td>requires</td><td>1</td></tr><tr><td>package.</td><td>1</td></tr><tr><td>clean</td><td>1</td></tr><tr><td>sc.parallelize(range(1000)).count()</td><td>1</td></tr><tr><td>high-level</td><td>1</td></tr><tr><td>against</td><td>1</td></tr><tr><td>through</td><td>1</td></tr><tr><td>package.)</td><td>1</td></tr><tr><td>easiest</td><td>1</td></tr><tr><td>no</td><td>1</td></tr><tr><td>Testing</td><td>1</td></tr><tr><td>several</td><td>1</td></tr><tr><td>help</td><td>1</td></tr><tr><td>The</td><td>1</td></tr><tr><td>sample</td><td>1</td></tr><tr><td>MASTER=spark://host:7077</td><td>1</td></tr><tr><td>Big</td><td>1</td></tr><tr><td>#</td><td>1</td></tr><tr><td>Online</td><td>1</td></tr><tr><td>usage</td><td>1</td></tr><tr><td>Spark\"](http://spark.apache.org/docs/latest/building-spark.html).</td><td>1</td></tr><tr><td>way</td><td>1</td></tr><tr><td>prefer</td><td>1</td></tr><tr><td>build/mvn</td><td>1</td></tr><tr><td>running</td><td>1</td></tr><tr><td>web</td><td>1</td></tr><tr><td>locally.</td><td>1</td></tr><tr><td>URL,</td><td>1</td></tr><tr><td>higher-level</td><td>1</td></tr><tr><td>tools</td><td>1</td></tr><tr><td>available</td><td>1</td></tr><tr><td>Documentation</td><td>1</td></tr><tr><td>this</td><td>1</td></tr><tr><td>tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).</td><td>1</td></tr><tr><td>(You</td><td>1</td></tr><tr><td>>>></td><td>1</td></tr><tr><td><http://spark.apache.org/></td><td>1</td></tr><tr><td>environment</td><td>1</td></tr><tr><td>built,</td><td>1</td></tr><tr><td>module,</td><td>1</td></tr><tr><td>them,</td><td>1</td></tr><tr><td>`./bin/run-example</td><td>1</td></tr><tr><td>instance:</td><td>1</td></tr><tr><td>first</td><td>1</td></tr><tr><td>documentation,</td><td>1</td></tr><tr><td>[params]`.</td><td>1</td></tr><tr><td>mesos://</td><td>1</td></tr><tr><td>engine</td><td>1</td></tr><tr><td>GraphX</td><td>1</td></tr><tr><td>example:</td><td>1</td></tr><tr><td>HDFS</td><td>1</td></tr><tr><td>individual</td><td>1</td></tr><tr><td>changed</td><td>1</td></tr><tr><td>./bin/spark-shell</td><td>1</td></tr><tr><td>threads.</td><td>1</td></tr><tr><td>storage</td><td>1</td></tr><tr><td>version</td><td>1</td></tr><tr><td>instructions.</td><td>1</td></tr><tr><td>Building</td><td>1</td></tr><tr><td>start</td><td>1</td></tr><tr><td>Many</td><td>1</td></tr><tr><td>And</td><td>1</td></tr><tr><td>distribution</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "",
         32
        ],
        [
         "the",
         21
        ],
        [
         "to",
         14
        ],
        [
         "Spark",
         13
        ],
        [
         "for",
         11
        ],
        [
         "and",
         10
        ],
        [
         "##",
         8
        ],
        [
         "a",
         8
        ],
        [
         "run",
         7
        ],
        [
         "can",
         6
        ],
        [
         "is",
         6
        ],
        [
         "in",
         5
        ],
        [
         "on",
         5
        ],
        [
         "of",
         5
        ],
        [
         "you",
         4
        ],
        [
         "if",
         4
        ],
        [
         "also",
         4
        ],
        [
         "documentation",
         3
        ],
        [
         "example",
         3
        ],
        [
         "with",
         3
        ],
        [
         "use",
         3
        ],
        [
         "You",
         3
        ],
        [
         "Please",
         3
        ],
        [
         "build",
         3
        ],
        [
         "an",
         3
        ],
        [
         "including",
         3
        ],
        [
         "or",
         3
        ],
        [
         "Hadoop",
         3
        ],
        [
         "command,",
         2
        ],
        [
         "set",
         2
        ],
        [
         "programs",
         2
        ],
        [
         "tests",
         2
        ],
        [
         "particular",
         2
        ],
        [
         "using",
         2
        ],
        [
         "guidance",
         2
        ],
        [
         "shell:",
         2
        ],
        [
         "Interactive",
         2
        ],
        [
         "how",
         2
        ],
        [
         "Hive",
         2
        ],
        [
         "one",
         2
        ],
        [
         "building",
         2
        ],
        [
         "be",
         2
        ],
        [
         "locally",
         2
        ],
        [
         "should",
         2
        ],
        [
         "[project",
         2
        ],
        [
         "`examples`",
         2
        ],
        [
         "general",
         2
        ],
        [
         "1000:",
         2
        ],
        [
         "detailed",
         2
        ],
        [
         "following",
         2
        ],
        [
         "SparkPi",
         2
        ],
        [
         "refer",
         2
        ],
        [
         "./bin/run-example",
         2
        ],
        [
         "It",
         2
        ],
        [
         "To",
         2
        ],
        [
         "return",
         2
        ],
        [
         "cluster",
         2
        ],
        [
         "do",
         2
        ],
        [
         "Scala",
         2
        ],
        [
         "class",
         2
        ],
        [
         "Hadoop,",
         2
        ],
        [
         "SQL",
         2
        ],
        [
         "Python,",
         2
        ],
        [
         "examples",
         2
        ],
        [
         "Python",
         2
        ],
        [
         "at",
         2
        ],
        [
         "that",
         2
        ],
        [
         "For",
         2
        ],
        [
         "This",
         2
        ],
        [
         "Shell",
         2
        ],
        [
         "supports",
         2
        ],
        [
         "which",
         2
        ],
        [
         "online",
         1
        ],
        [
         "graphs",
         1
        ],
        [
         "[\"Building",
         1
        ],
        [
         "abbreviated",
         1
        ],
        [
         "overview",
         1
        ],
        [
         "rich",
         1
        ],
        [
         "-DskipTests",
         1
        ],
        [
         "name",
         1
        ],
        [
         "[\"Specifying",
         1
        ],
        [
         "stream",
         1
        ],
        [
         "run:",
         1
        ],
        [
         "not",
         1
        ],
        [
         "./dev/run-tests",
         1
        ],
        [
         "will",
         1
        ],
        [
         "[run",
         1
        ],
        [
         "Alternatively,",
         1
        ],
        [
         "must",
         1
        ],
        [
         "MLlib",
         1
        ],
        [
         "DataFrames,",
         1
        ],
        [
         "variable",
         1
        ],
        [
         "Note",
         1
        ],
        [
         "core",
         1
        ],
        [
         "protocols",
         1
        ],
        [
         "site,",
         1
        ],
        [
         "systems.",
         1
        ],
        [
         "[building",
         1
        ],
        [
         "configure",
         1
        ],
        [
         "README",
         1
        ],
        [
         "[Configuration",
         1
        ],
        [
         "system",
         1
        ],
        [
         "provides",
         1
        ],
        [
         "Hadoop-supported",
         1
        ],
        [
         "pre-built",
         1
        ],
        [
         "directory.",
         1
        ],
        [
         "wiki](https://cwiki.apache.org/confluence/display/SPARK).",
         1
        ],
        [
         "Example",
         1
        ],
        [
         "MASTER",
         1
        ],
        [
         "library",
         1
        ],
        [
         "Spark.",
         1
        ],
        [
         "contains",
         1
        ],
        [
         "Configuration",
         1
        ],
        [
         "programming",
         1
        ],
        [
         "downloaded",
         1
        ],
        [
         "1000).count()",
         1
        ],
        [
         "comes",
         1
        ],
        [
         "machine",
         1
        ],
        [
         "Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)",
         1
        ],
        [
         "params",
         1
        ],
        [
         "Guide](http://spark.apache.org/docs/latest/configuration.html)",
         1
        ],
        [
         "given.",
         1
        ],
        [
         "same",
         1
        ],
        [
         "page](http://spark.apache.org/documentation.html)",
         1
        ],
        [
         "Programs",
         1
        ],
        [
         "using:",
         1
        ],
        [
         "fast",
         1
        ],
        [
         "[Apache",
         1
        ],
        [
         "your",
         1
        ],
        [
         "optimized",
         1
        ],
        [
         "R,",
         1
        ],
        [
         "graph",
         1
        ],
        [
         "package",
         1
        ],
        [
         "project",
         1
        ],
        [
         "versions",
         1
        ],
        [
         "Spark](#building-spark).",
         1
        ],
        [
         "other",
         1
        ],
        [
         "learning,",
         1
        ],
        [
         "when",
         1
        ],
        [
         "submit",
         1
        ],
        [
         "Apache",
         1
        ],
        [
         "About",
         1
        ],
        [
         "scala>",
         1
        ],
        [
         "print",
         1
        ],
        [
         "different",
         1
        ],
        [
         "data",
         1
        ],
        [
         "Tests",
         1
        ],
        [
         "Versions",
         1
        ],
        [
         "Data.",
         1
        ],
        [
         "processing.",
         1
        ],
        [
         "its",
         1
        ],
        [
         "basic",
         1
        ],
        [
         "latest",
         1
        ],
        [
         "only",
         1
        ],
        [
         "<class>",
         1
        ],
        [
         "have",
         1
        ],
        [
         "runs.",
         1
        ],
        [
         "YARN,",
         1
        ],
        [
         "\"local\"",
         1
        ],
        [
         "processing,",
         1
        ],
        [
         "built",
         1
        ],
        [
         "Pi",
         1
        ],
        [
         "thread,",
         1
        ],
        [
         "A",
         1
        ],
        [
         "APIs",
         1
        ],
        [
         "Scala,",
         1
        ],
        [
         "file",
         1
        ],
        [
         "computation",
         1
        ],
        [
         "Once",
         1
        ],
        [
         "find",
         1
        ],
        [
         "sc.parallelize(1",
         1
        ],
        [
         "uses",
         1
        ],
        [
         "N",
         1
        ],
        [
         "programs,",
         1
        ],
        [
         "\"yarn\"",
         1
        ],
        [
         "see",
         1
        ],
        [
         "./bin/pyspark",
         1
        ],
        [
         "computing",
         1
        ],
        [
         "Java,",
         1
        ],
        [
         "from",
         1
        ],
        [
         "Because",
         1
        ],
        [
         "Streaming",
         1
        ],
        [
         "More",
         1
        ],
        [
         "analysis.",
         1
        ],
        [
         "Maven](http://maven.apache.org/).",
         1
        ],
        [
         "cluster.",
         1
        ],
        [
         "Running",
         1
        ],
        [
         "talk",
         1
        ],
        [
         "distributions.",
         1
        ],
        [
         "guide,",
         1
        ],
        [
         "\"local[N]\"",
         1
        ],
        [
         "Try",
         1
        ],
        [
         "setup",
         1
        ],
        [
         "need",
         1
        ],
        [
         "spark://",
         1
        ],
        [
         "Thriftserver",
         1
        ],
        [
         "are",
         1
        ],
        [
         "requires",
         1
        ],
        [
         "package.",
         1
        ],
        [
         "clean",
         1
        ],
        [
         "sc.parallelize(range(1000)).count()",
         1
        ],
        [
         "high-level",
         1
        ],
        [
         "against",
         1
        ],
        [
         "through",
         1
        ],
        [
         "package.)",
         1
        ],
        [
         "easiest",
         1
        ],
        [
         "no",
         1
        ],
        [
         "Testing",
         1
        ],
        [
         "several",
         1
        ],
        [
         "help",
         1
        ],
        [
         "The",
         1
        ],
        [
         "sample",
         1
        ],
        [
         "MASTER=spark://host:7077",
         1
        ],
        [
         "Big",
         1
        ],
        [
         "#",
         1
        ],
        [
         "Online",
         1
        ],
        [
         "usage",
         1
        ],
        [
         "Spark\"](http://spark.apache.org/docs/latest/building-spark.html).",
         1
        ],
        [
         "way",
         1
        ],
        [
         "prefer",
         1
        ],
        [
         "build/mvn",
         1
        ],
        [
         "running",
         1
        ],
        [
         "web",
         1
        ],
        [
         "locally.",
         1
        ],
        [
         "URL,",
         1
        ],
        [
         "higher-level",
         1
        ],
        [
         "tools",
         1
        ],
        [
         "available",
         1
        ],
        [
         "Documentation",
         1
        ],
        [
         "this",
         1
        ],
        [
         "tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).",
         1
        ],
        [
         "(You",
         1
        ],
        [
         ">>>",
         1
        ],
        [
         "<http://spark.apache.org/>",
         1
        ],
        [
         "environment",
         1
        ],
        [
         "built,",
         1
        ],
        [
         "module,",
         1
        ],
        [
         "them,",
         1
        ],
        [
         "`./bin/run-example",
         1
        ],
        [
         "instance:",
         1
        ],
        [
         "first",
         1
        ],
        [
         "documentation,",
         1
        ],
        [
         "[params]`.",
         1
        ],
        [
         "mesos://",
         1
        ],
        [
         "engine",
         1
        ],
        [
         "GraphX",
         1
        ],
        [
         "example:",
         1
        ],
        [
         "HDFS",
         1
        ],
        [
         "individual",
         1
        ],
        [
         "changed",
         1
        ],
        [
         "./bin/spark-shell",
         1
        ],
        [
         "threads.",
         1
        ],
        [
         "storage",
         1
        ],
        [
         "version",
         1
        ],
        [
         "instructions.",
         1
        ],
        [
         "Building",
         1
        ],
        [
         "start",
         1
        ],
        [
         "Many",
         1
        ],
        [
         "And",
         1
        ],
        [
         "distribution",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "word",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "word_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_count.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "624bb24c-4ed5-467d-abf6-8b73fb31fb93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1385520120954422,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "001 Word Count using Pyspark Dataframe APIs - Developing the logic",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
